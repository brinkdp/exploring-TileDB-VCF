{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17af7e73-a138-441f-a62b-bfb00b93d283",
   "metadata": {},
   "source": [
    "# Exploring TileDB-VCF: \n",
    "# Notebook 2. A deeper look at ingestion of data\n",
    "2024-12-20 Daniel P. Brink\n",
    "\n",
    "This notebook investigates various aspects of data ingestion with the TileDB-VCF python library. It compares loading of tutorial data from a S3 bucket versus local file, finds (through some trial-and-error) preferrable ways of handling file ingestion, and investigates how multi-sample files can be handled to allow their data to be ingested.\n",
    "\n",
    "The approach of this notebook is to jump head-first into the commands with little prior knowledge other than that which was learned in notedbook 1 of this GitHub repo. It is very likely that some of the issue encountered below could have been circumented had I read the manual more closely, beforehand. However, by working in this manner, I stumbled upon aspects about how TileDB-VCF handles duplicate ingestion of the same sample multiple times. This lead to some interesting observations, and suggestions for how to work with data ingestion with this library.\n",
    "\n",
    "The timings in the saved output in this notebook should be taken with a grain of salt, as they were run on a laptop and not a dedicated computing environment. Absolute numbers aside, the relative trends between different methods should be of interest. Another thing to keep in mind is that this notebook used the TileDB-VCF tutorial data, which consists of VCF files from chr1 of the 1000 genomes project for five individuals. This is to be considered a small dataset, as far as variant calling data goes. This means that the findings of this notebook might be completely different for a larger dataset.\n",
    "\n",
    "**Key findings:**\n",
    "- The TileDB-VCF python libary does by default not really communicate warnings or error messages. Wrapping the functions in `try-except`blocks helps a lot for debugging\n",
    "- Duplicate ingestion of the same sample is possible, and can easily be done by mistake (especially when working in a Jupyter notebook and rerunning individual cells). Wrapping the ingestion method in a conditional that checks which samples are already ingested is likely a good practice.\n",
    "- As already stated in the TileDB-VCF tutorials, ingestion of the example data from local files was substantially faster than ingestion from S3 bucket.\n",
    "- Ingestion of multiple samples is faster when the `ds.ingest_sample()` is given a list of all samples compared to when it is given one sample at a time. This is perhaps not very surprising, but good to keep in mind when trying to optimize the work-flow.\n",
    "- Since TileDB-VCF does not support ingestion of multi-sample VCFs, such files must be split prior to ingestion (e.g. with `bcftools`). This creates an processing overhead that, while unremarkable for the tutorial files, might be massive for large datasets. More testing will be required to learn more about how this scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ba9be-6898-43da-9617-39d227453e53",
   "metadata": {},
   "source": [
    "# 1. Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "e9768304-3abf-4a28-808e-80dfa4b20b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Conda environment: tileDB_vcf\n",
      "tiledb v0.31.1\n",
      "numpy v1.26.4\n",
      "tiledb-vcf v0.34.2\n",
      "\n",
      "bcftools 1.20\n",
      "Using htslib 1.20\n",
      "Copyright (C) 2024 Genome Research Ltd.\n",
      "License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n",
      "This is free software: you are free to change and redistribute it.\n",
      "There is NO WARRANTY, to the extent permitted by law.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import tiledb\n",
    "import tiledbvcf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time\n",
    "import statistics\n",
    "import contextlib\n",
    "import io\n",
    "from io import StringIO\n",
    "import subprocess\n",
    "\n",
    "\n",
    "#Check that Conda and the libraries are installed as expected:\n",
    "print(f\"Current Conda environment: {os.environ['CONDA_DEFAULT_ENV']}\")\n",
    "\n",
    "print(\n",
    "    f\"tiledb v{tiledb.version.version}\\n\"\n",
    "    f\"numpy v{np.__version__}\\n\"\n",
    "    f\"tiledb-vcf v{tiledbvcf.version}\\n\"\n",
    ")\n",
    "\n",
    "!bcftools --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b02bf2-1dab-45f7-92cb-8c996766fba7",
   "metadata": {},
   "source": [
    "# 2. Ingest some data from S3 bucket and export it to local disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1053fa68-389e-4098-99e5-a4166fc0f01b",
   "metadata": {},
   "source": [
    "One of the main [tutorials for TileDB-VCF](https://tiledb-inc.github.io/TileDB-VCF/examples/tutorial_tiledbvcf_basics.html) mention that ingestion of data from a cloud bucket is going to be slower than ingesting it from disk. It would be interesting to compare time it takes to do the two. In order to have a fair comparison, let's ingest the .bcf data from the cloud, and then export to local .bcf files on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e29d46-afe6-47e0-a364-545cfb637528",
   "metadata": {},
   "source": [
    "Initiate storage for the TileDB array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "093e823d-96f1-409d-8beb-5959a8d5b920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing array './temp_array_storage/ingestion_testing'\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__meta',\n",
       " 'variant_stats',\n",
       " 'allele_count',\n",
       " 'sample_stats',\n",
       " 'data',\n",
       " '__tiledb_group.tdb',\n",
       " 'metadata',\n",
       " '__group']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vfs = tiledb.VFS(config=tiledb.Config())\n",
    "array_uri = \"./temp_array_storage/ingestion_testing\"\n",
    "if (vfs.is_dir(array_uri)):\n",
    "    print(f\"Deleting existing array '{array_uri}'\")\n",
    "    vfs.remove_dir(array_uri)\n",
    "    print(\"Done.\")\n",
    "ds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\")\n",
    "ds\n",
    "ds.create_dataset(enable_allele_count=True, enable_variant_stats=True)\n",
    "\n",
    "# Verify that the array exists\n",
    "os.listdir(array_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f42886-478b-41e0-a0bc-14345bf5e090",
   "metadata": {},
   "source": [
    "Like in the previous notebook, we will use the sample data from the 1000 genomes project supplied by TileDB from their S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c24abcc0-e533-4f36-b032-ad70641e87d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.5 s, sys: 4.25 s, total: 20.7 s\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vcf_bucket = \"s3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1\"\n",
    "batch1_samples = [\"HG00096.bcf\", \"HG00097.bcf\", \"HG00099.bcf\", \"HG00100.bcf\", \"HG00101.bcf\"]\n",
    "batch1_uris = [f\"{vcf_bucket}/{s}\" for s in batch1_samples]\n",
    "\n",
    "ds.ingest_samples(sample_uris = batch1_uris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c5162-9e68-4250-9d15-8e650e778f32",
   "metadata": {},
   "source": [
    "Let's look at the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "325d3c7b-952b-4030-a573-40ecadf850a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Sample names can only be retrieved for reader",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tileDB_vcf/lib/python3.12/site-packages/tiledbvcf/dataset.py:909\u001b[0m, in \u001b[0;36mDataset.samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;124;03mGet the list of samples in the dataset.\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;124;03m    List of samples in the dataset.\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample names can only be retrieved for reader\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader\u001b[38;5;241m.\u001b[39mget_sample_names()\n",
      "\u001b[0;31mException\u001b[0m: Sample names can only be retrieved for reader"
     ]
    }
   ],
   "source": [
    "ds.samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e20baf2-67aa-4ae3-bdca-87b8c273f3e6",
   "metadata": {},
   "source": [
    "Interestingly, the dataset is locked for operations unless it is set to read or write mode. We need to change it back to read mode first. (There is going to be a lot of back-and-forth with this in this notebook...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdc88313-0816-43b0-b15e-827ba386a2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HG00096', 'HG00097', 'HG00099', 'HG00100', 'HG00101']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tiledbvcf.Dataset(array_uri, mode = \"r\")\n",
    "ds.samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde22c5-3f47-4132-8340-02bd3db7d205",
   "metadata": {},
   "source": [
    "Let's export the dataset to disk as single-samples files so that we can later compare ingestion speeds between the TileDB S3 bucket and the local files. The files were in BCF format in the bucket, but we can just as well export to VCF as well to compare the size difference between BCF and VCF versions of the same file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a6c09af-5720-4100-9f41-17ba9d6b4146",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"temp_VCF_exports/untruncated_files\", exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33bc3324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 s, sys: 953 ms, total: 11.5 s\n",
      "Wall time: 8.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for sample in batch1_samples:\n",
    "    sample=sample.rstrip(\".bcf\")\n",
    "    ds.export(\n",
    "        samples = [sample],\n",
    "        output_format = 'b',\n",
    "        output_dir = 'temp_VCF_exports/untruncated_files'\n",
    "    )\n",
    "    ds.export(\n",
    "        samples = [sample],\n",
    "        output_format = 'v',\n",
    "        output_dir = 'temp_VCF_exports/untruncated_files'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa3ae1c-21f4-46f5-93fd-9f5ec19a124d",
   "metadata": {},
   "source": [
    "(Sanity-check: the expected behaviour is that the export method should not include variants that have empty genotype calls for a given sample. The VCF notation for that is `.|.`. Thus, the following command should return empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "e7d46198-fb1f-4c72-9e3a-c906a2a8a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bcftools view temp_VCF_exports/untruncated_files/HG00099.bcf | grep -F '.|.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c798a90-dbfe-46bb-9a41-3629590aff65",
   "metadata": {},
   "source": [
    "Which it did. Good.)\n",
    "\n",
    "As expected, the BCF version is a fraction of the size of the VCF version of the same data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f33ffee5-4907-4676-8d05-9bc1ecf0f00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_VCF_exports/untruncated_files/HG00101.bcf\t6.19 Mb\n",
      "temp_VCF_exports/untruncated_files/HG00100.bcf\t6.24 Mb\n",
      "temp_VCF_exports/untruncated_files/HG00099.bcf\t6.14 Mb\n",
      "temp_VCF_exports/untruncated_files/HG00096.vcf\t27.92 Mb\n",
      "temp_VCF_exports/untruncated_files/HG00097.vcf\t28.53 Mb\n",
      "temp_VCF_exports/untruncated_files/HG00099.vcf\t27.90 Mb\n",
      "temp_VCF_exports/untruncated_files/HG00100.vcf\t28.34 Mb\n",
      "temp_VCF_exports/untruncated_files/HG00101.vcf\t28.11 Mb\n",
      "temp_VCF_exports/untruncated_files/HG00097.bcf\t6.28 Mb\n",
      "temp_VCF_exports/untruncated_files/HG00096.bcf\t6.14 Mb\n"
     ]
    }
   ],
   "source": [
    "file_path = \"temp_VCF_exports/untruncated_files\"\n",
    "vcf_files = glob.glob(os.path.join(file_path, \"*.*cf\"))\n",
    "\n",
    "for file in vcf_files:\n",
    "    file_size_bytes = os.path.getsize(file)\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "    print(f\"{file}\\t{file_size_mb:.2f} Mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce2482-0158-47b9-871b-e11b3f72bded",
   "metadata": {},
   "source": [
    "All seemed to work fine. But while working on the code in section 1, I noticed that the `ds.export()`command seemed to run slower if I had iterated over the ingestion command without first having re-initated the dataset. Because of this observation, we will now digress from the intended benchmarking for little bit to investigate how duplicate samples are handled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c2e3d-6f30-4bd1-af0b-7ca3308c36bc",
   "metadata": {},
   "source": [
    "# 3. Single sample ingestion: how are duplicate samples handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1afeb89-1efa-4a5b-a8b4-157bca69d12b",
   "metadata": {},
   "source": [
    "While working on section 2, it seemed that rerunning the `.ingest_samples()` command with the same samples seemed to rewrite the dataset without any warning message. This made me suspicious about how TileDB-VCF handles duplicate rows. We will now investigate this in more detail.\n",
    "\n",
    "Granted, I am currently working on this in a Jupyter notebook, rerunning code-blocks and not the whole script. This means that the TileDB-VCF dataset is not automatically re-initated everytime the code is run. For an script implementation of TileDB-VCF, it is less likely that such duplication issues occurs, as the TileDB-VCF array will probably be re-initated in each iteration. But it is still worth learning more about the behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba7733-7c8c-40b7-9d90-f5d02705d8ee",
   "metadata": {},
   "source": [
    "## 3.1. Understanding the behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50eebc17-0ffa-4bac-8ec4-32083c93e1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing array './temp_array_storage/ingestion_testing'\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__meta',\n",
       " 'variant_stats',\n",
       " 'allele_count',\n",
       " 'sample_stats',\n",
       " 'data',\n",
       " '__tiledb_group.tdb',\n",
       " 'metadata',\n",
       " '__group']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (vfs.is_dir(array_uri)):\n",
    "    print(f\"Deleting existing array '{array_uri}'\")\n",
    "    vfs.remove_dir(array_uri)\n",
    "    print(\"Done.\")\n",
    "ds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\")\n",
    "ds\n",
    "ds.create_dataset(enable_allele_count=True, enable_variant_stats=True)\n",
    "\n",
    "# Verify that the array exists\n",
    "os.listdir(array_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527ea45-5ed9-442a-81e6-de823390f759",
   "metadata": {},
   "source": [
    "Start by ingesting a single sample from S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8682847d-06ef-4be9-ab72-bb36eab0320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_bucket = \"s3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1\"\n",
    "single_sample_uri = [f\"{vcf_bucket}/HG00096.bcf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a0b4734-db85-4bff-aff0-4ac48eee3952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.55 s, sys: 1.22 s, total: 4.77 s\n",
      "Wall time: 31.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.ingest_samples(sample_uris = single_sample_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b403daa-cee6-4080-a8bf-cb040e1c6adf",
   "metadata": {},
   "source": [
    "What happens if we ingest the same sample twice? Will it overwrite the previous data (which I what I assume, given that there are scaffold coordinates for each row), or will it append the duplicate lines? We can for instance investigate this by creating pandas dataframes from the TileDB dataset before and after duplicate ingestion. Just for fun, let's also export the files to VCF.\n",
    "\n",
    "Interestingly, the `.export()` method can only export files to a specific file name when the `merge=True` flag is set. This flag is intended for combined VCFs (multi-sample VCFs), and uses `output_path` instead of `output_dir`. But here we can make a small hack to export a single file this way by asking for a merged file for only one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27a99723-016d-45c5-ae5b-ab3d833f0148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 s, sys: 238 ms, total: 16.5 s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = tiledbvcf.Dataset(array_uri, mode = \"r\")\n",
    "test_df1 = ds.read(\n",
    "    samples=[\"HG00096\"],\n",
    "    attrs=[\n",
    "        \"sample_name\",\n",
    "        \"contig\",\n",
    "        \"pos_start\",\n",
    "        \"pos_end\",\n",
    "        \"alleles\",\n",
    "        \"fmt_GT\",\n",
    "    ],\n",
    ")\n",
    "for sample in batch1_samples:\n",
    "    sample=sample.rstrip(\".bcf\")\n",
    "    ds.export(\n",
    "        samples = [sample],\n",
    "        merge = True,\n",
    "        output_format = 'v',\n",
    "        output_path = f\"temp_VCF_exports/untruncated_files/test_{sample}.vcf\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8b38869-47c9-467b-b88d-a564c618604c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_name</th>\n",
       "      <th>contig</th>\n",
       "      <th>pos_start</th>\n",
       "      <th>pos_end</th>\n",
       "      <th>alleles</th>\n",
       "      <th>fmt_GT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>10177</td>\n",
       "      <td>10177</td>\n",
       "      <td>[A, AC]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>10352</td>\n",
       "      <td>10352</td>\n",
       "      <td>[T, TA]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>10616</td>\n",
       "      <td>10637</td>\n",
       "      <td>[CCGCCGTTGCAAAGGCGCGCCG, C]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>14464</td>\n",
       "      <td>14464</td>\n",
       "      <td>[A, T]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>14930</td>\n",
       "      <td>14930</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320469</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>249240051</td>\n",
       "      <td>249240051</td>\n",
       "      <td>[T, TA]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320470</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>249240099</td>\n",
       "      <td>249240099</td>\n",
       "      <td>[T, TA]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320471</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>249240219</td>\n",
       "      <td>249240219</td>\n",
       "      <td>[A, T]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320472</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>249240537</td>\n",
       "      <td>249240539</td>\n",
       "      <td>[GGT, G]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320473</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>249240539</td>\n",
       "      <td>249240539</td>\n",
       "      <td>[T, G]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320474 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_name contig  pos_start    pos_end                      alleles  \\\n",
       "0          HG00096      1      10177      10177                      [A, AC]   \n",
       "1          HG00096      1      10352      10352                      [T, TA]   \n",
       "2          HG00096      1      10616      10637  [CCGCCGTTGCAAAGGCGCGCCG, C]   \n",
       "3          HG00096      1      14464      14464                       [A, T]   \n",
       "4          HG00096      1      14930      14930                       [A, G]   \n",
       "...            ...    ...        ...        ...                          ...   \n",
       "320469     HG00096      1  249240051  249240051                      [T, TA]   \n",
       "320470     HG00096      1  249240099  249240099                      [T, TA]   \n",
       "320471     HG00096      1  249240219  249240219                       [A, T]   \n",
       "320472     HG00096      1  249240537  249240539                     [GGT, G]   \n",
       "320473     HG00096      1  249240539  249240539                       [T, G]   \n",
       "\n",
       "        fmt_GT  \n",
       "0       [1, 0]  \n",
       "1       [1, 0]  \n",
       "2       [1, 1]  \n",
       "3       [1, 1]  \n",
       "4       [1, 0]  \n",
       "...        ...  \n",
       "320469  [0, 1]  \n",
       "320470  [1, 0]  \n",
       "320471  [1, 0]  \n",
       "320472  [1, 0]  \n",
       "320473  [0, 1]  \n",
       "\n",
       "[320474 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15c4f9-51f3-41ac-bd9c-bb7a7fbcb3e4",
   "metadata": {},
   "source": [
    "Now, let's ingest the same sample once again and learn how TileDB handles duplicate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a602c31-3705-477a-b9b0-26a7e5f6454e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.77 s, sys: 1.14 s, total: 4.91 s\n",
      "Wall time: 30 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = tiledbvcf.Dataset(array_uri, mode = \"w\")\n",
    "ds.ingest_samples(sample_uris = batch1_uris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e51e8-d34b-4bd7-b624-bcd0562bd5b5",
   "metadata": {},
   "source": [
    "Interestingly, the time to ingest a new sample is about the same as the first sample (this operation is what TileDB is supposedly good at), but when we export it to df and file, it is clear that the repeated ingestion has, infact, led to longer processing times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62d7d8d3-e474-4e9e-99b7-ba4850945cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 22s, sys: 750 ms, total: 2min 23s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = tiledbvcf.Dataset(array_uri, mode = \"r\")\n",
    "test_df2 = ds.read(\n",
    "    samples=[\"HG00096\"],\n",
    "    attrs=[\n",
    "        \"sample_name\",\n",
    "        \"contig\",\n",
    "        \"pos_start\",\n",
    "        \"pos_end\",\n",
    "        \"alleles\",\n",
    "        \"fmt_GT\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "for sample in batch1_samples:\n",
    "    sample=sample.rstrip(\".bcf\")\n",
    "    ds.export(\n",
    "        samples = [sample],\n",
    "        merge = True,\n",
    "        output_format = 'v',\n",
    "        output_path = f\"temp_VCF_exports/untruncated_files/test_{sample}2.vcf\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14600c3f-f32d-4a55-8734-05c80baa9d69",
   "metadata": {},
   "source": [
    "Comparing the number of rows and columns in the two dataframes clearly show that the data from the second ingestion (the duplicate) was appended to the file. This is not the the behaviour I expected from parser for a co-ordinate based dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "d406ce67-124e-429b-8a4f-85ee4461a9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The single-ingestion (test_df1) contains:\t 320474 rows, 6 columns.\n",
      "The duplicate-ingestion (test_df2) contains:\t 640948 rows, 6 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The single-ingestion (test_df1) contains:\\t {(rows := test_df1.shape[0])} rows, {(columns := test_df1.shape[1])} columns.\")\n",
    "print(f\"The duplicate-ingestion (test_df2) contains:\\t {(rows := test_df2.shape[0])} rows, {(columns := test_df2.shape[1])} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8411b698-195b-4f00-86c5-f75c478a1293",
   "metadata": {},
   "source": [
    "Given the assumption that each chromosomal position only has a single entry in the original BCF, we can check to see if the first row of `test_df1` is duplicated in `test_df2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9318f3fd-f0a9-4deb-a54a-7f68b600c685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_name</th>\n",
       "      <th>contig</th>\n",
       "      <th>pos_start</th>\n",
       "      <th>pos_end</th>\n",
       "      <th>alleles</th>\n",
       "      <th>fmt_GT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>10177</td>\n",
       "      <td>10177</td>\n",
       "      <td>[A, AC]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_name contig  pos_start  pos_end  alleles  fmt_GT\n",
       "0     HG00096      1      10177    10177  [A, AC]  [1, 0]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df1[test_df1[\"pos_start\"] == 10177]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8139445a-0e22-4a4f-8234-e2646ca77566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_name</th>\n",
       "      <th>contig</th>\n",
       "      <th>pos_start</th>\n",
       "      <th>pos_end</th>\n",
       "      <th>alleles</th>\n",
       "      <th>fmt_GT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>10177</td>\n",
       "      <td>10177</td>\n",
       "      <td>[A, AC]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320474</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>10177</td>\n",
       "      <td>10177</td>\n",
       "      <td>[A, AC]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_name contig  pos_start  pos_end  alleles  fmt_GT\n",
       "0          HG00096      1      10177    10177  [A, AC]  [1, 0]\n",
       "320474     HG00096      1      10177    10177  [A, AC]  [1, 0]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2[test_df2[\"pos_start\"] == 10177]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1838ce-b080-47a8-8a7c-eb1c76f49916",
   "metadata": {},
   "source": [
    "Indeed it is. And the duplication is on row 320474, which is one row below the last row of `test_df1` (320474). As the pandas index implies, it seem that TileDB-VCF does not take the values of the duplicated rows into consideration. Instead it just appends the duplicate, storing it as another row object in the dataset. Perhaps this is just how the pandas dataframe instance of the TileDB array (ds) is organized? If two samples have their own unique variant in the same position, would the dataframe handle that as two different rows? The examples from the previous notebook seem to imply that.\n",
    "\n",
    "(However, if we were to add a different additional sample, we would expect that the `ds.export()` will rewrite the data on a row basis (adding another sample column to each row).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8ee10-b350-4bd8-b718-c1611af046f9",
   "metadata": {},
   "source": [
    "It is always good practice to verify the findings with an additional method. For what it is worth, we can also see that the duplication is also reflected in the file size of the exported files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c74039fa-4d4b-4bc1-85c4-b6b3a423a615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_VCF_exports/untruncated_files/test_HG00096.vcf\t27.92 Mb\n",
      "temp_VCF_exports/untruncated_files/test_HG000962.vcf\t55.83 Mb\n"
     ]
    }
   ],
   "source": [
    "file_path = \"temp_VCF_exports/untruncated_files\"\n",
    "vcf_files = glob.glob(os.path.join(file_path, \"test*.*cf\"))\n",
    "\n",
    "for file in vcf_files:\n",
    "    file_size_bytes = os.path.getsize(file)\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "    print(f\"{file}\\t{file_size_mb:.2f} Mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54572850-cacf-4400-9721-485c9cd999ca",
   "metadata": {},
   "source": [
    "## 3.2 A suggestion as for how to avoid ingesting duplicate samples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf257d-ece8-4028-9ab6-c3e8a6705c0c",
   "metadata": {},
   "source": [
    "One way to avoid it duplicate ingestions to declare new objects `ds`, `ds2`, etc. for each ingestion. Another way, that is perhaps even better (?), would be to start up a new array_uri every time. This behaviour is probably also a reason why the read-write permissions on the dataset is strict.\n",
    "\n",
    "But to prevent this from happening, we could implement a conditional check that ensures that samples cannot be added twice to the TileDB-VCF array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c9d0be29-5818-432b-a825-9c9e1ecdc507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing array './temp_array_storage/ingestion_testing'\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__meta',\n",
       " 'variant_stats',\n",
       " 'allele_count',\n",
       " 'sample_stats',\n",
       " 'data',\n",
       " '__tiledb_group.tdb',\n",
       " 'metadata',\n",
       " '__group']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (vfs.is_dir(array_uri)):\n",
    "    print(f\"Deleting existing array '{array_uri}'\")\n",
    "    vfs.remove_dir(array_uri)\n",
    "    print(\"Done.\")\n",
    "ds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\")\n",
    "ds\n",
    "ds.create_dataset(enable_allele_count=True, enable_variant_stats=True)\n",
    "\n",
    "# Verify that the array exists\n",
    "os.listdir(array_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2691ddd7-2b06-4e2a-901d-39909c61c317",
   "metadata": {},
   "source": [
    "Quick check to ensure that the re-initated dataset object contains no samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0aea0306-0aca-4029-9844-4364b63b3620",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tiledbvcf.Dataset(uri=array_uri, mode=\"r\")\n",
    "ds.samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdfb3bf-e0ec-4688-81ac-ec4747edbdbe",
   "metadata": {},
   "source": [
    "We can ensure that doublet ingestion does not occur by wrapping it in a logic sequence that uses `ds.samples()` to check which samples are alredy ingested in `ds`. It turns out try-except is needed to get error messages out of `ds.ingest_samples()`, so we can add that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b708e433-27f7-4bda-bc02-b9448c4e19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_samples_without_risk_of_duplication(new_samples: list):\n",
    "    ds = tiledbvcf.Dataset(uri=array_uri, mode=\"r\")\n",
    "    samples_to_ingest = [sample for sample in new_samples if os.path.basename(sample).rstrip(\".bcf\") not in (existing_samples := list(ds.samples()))]\n",
    "    if samples_to_ingest:\n",
    "        ds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\")\n",
    "        try:\n",
    "            ds.ingest_samples(samples_to_ingest)\n",
    "            print(f\"Successfully ingested sample: {samples_to_ingest}\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Failed to ingest sample '{samples_to_ingest}': {e}\")\n",
    "        print(f\"Ingested samples: {samples_to_ingest}\")\n",
    "    else:\n",
    "        print(\"No new samples to ingest: all samples are already present in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "edf89b9b-f4d9-4723-8e77-0e8b13f58945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested samples: ['s3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00096.bcf']\n",
      "CPU times: user 3.95 s, sys: 1.28 s, total: 5.22 s\n",
      "Wall time: 37.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vcf_bucket = \"s3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1\"\n",
    "new_samples = [f\"{vcf_bucket}/HG00096.bcf\"]\n",
    "\n",
    "ingest_samples_without_risk_of_duplication(new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ac802-6dd4-40af-b9b5-1438ed3a0ce4",
   "metadata": {},
   "source": [
    "To show that this function works for adding new samples that are not duplicates, we can test the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a4fff271-6b4b-48d5-841d-e66aca3a84fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested samples: ['s3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00097.bcf']\n",
      "CPU times: user 3.95 s, sys: 1.17 s, total: 5.12 s\n",
      "Wall time: 33.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vcf_bucket = \"s3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1\"\n",
    "new_samples = [f\"{vcf_bucket}/HG00096.bcf\",f\"{vcf_bucket}/HG00097.bcf\"]\n",
    "\n",
    "ingest_samples_without_risk_of_duplication(new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3bd83b-11d1-4ec3-b013-86de76daa74b",
   "metadata": {},
   "source": [
    "This last step should ingest the new sample and ignore the first sample. Which it did. Good!\n",
    "\n",
    "An additional sanity-check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eeaca53c-1346-46c5-abb0-3818a66e91dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HG00096', 'HG00097']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tiledbvcf.Dataset(uri=array_uri, mode=\"r\")\n",
    "ds.samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879cda5d-04ea-451b-9875-3e24be73e985",
   "metadata": {},
   "source": [
    "So, to iterate: the occurance of the duplication issue is likely to be most prominent in a Jupyter notebook work-flow, and might be alleviated by a script work-flow where the TileDB array is re-initiated upon each iteration. Still, the suggested solution will probably be useful also in that case to catch this issue if it was to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad585bee-113b-428b-9f6f-b31d1864972f",
   "metadata": {},
   "source": [
    "# 4. Comparing the time required to load the S3 tutorial files vs their local versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7017e-fcfc-437a-8069-3a2278f77047",
   "metadata": {},
   "source": [
    "OK, now that we know how to ensure that ingestion is not duplicated upon iteration of the code blocks, let's go back to the original question: how much slower is it to load the files from S3 than from their local exported versions?\n",
    "\n",
    "TileDB-VCF requires BCF files to be indexed prior to ingestion. This can be done with `bcftools`, which is the gold standard for massaging BCF files. The index files are saved as `[FILENAME].bcf.csi`, and, as one would expect, need to be in the same directory as their associated BCF file in order to be ingested by TileDB-VCF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "6b382c69-41cd-4f17-91bd-4d73a4393ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_local_dir = \"temp_VCF_exports/untruncated_files\"\n",
    "sample_list = [\"HG00096.bcf\", \"HG00097.bcf\", \"HG00099.bcf\", \"HG00100.bcf\", \"HG00101.bcf\"]\n",
    "new_samples = [f\"{path_to_local_dir}/{s}\" for s in sample_list]\n",
    "for file in new_samples:\n",
    "    !bcftools index {file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4008a6f-7641-49c4-b274-7033f68d2abf",
   "metadata": {},
   "source": [
    "The `%%time` magic command does not support saving its output to variable, so let's implement another way of timing the commands using the `time` python library. To get some basic statistics on how this performs on the current machine, we can wrap the code in a loop and make, say, ten iterations and calculate the average and standard deviation of the processing time. \n",
    "\n",
    "Note! The line `with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):` is added to suppress the messages from `ingest_samples_without_risk_of_duplication()`. This is perhaps a little risky behaviour, since the messages are very useful. But for the iterative benchmarking, let's allow us the pleasure to silence the messages. \n",
    "\n",
    "First, we assess the time it takes to ingest the local samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1138cc4e-d2dd-415e-ac3a-a97e33ac87b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 1: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 2: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 3: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 4: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 5: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 6: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 7: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 8: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 9: creating new array './temp_array_storage/ingestion_testing'\n",
      "It took 8.64 +/- 0.26 s (10 iterations)\n"
     ]
    }
   ],
   "source": [
    "timing_results = []\n",
    "number_of_iterations = 10\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    start_time = time.time()\n",
    "    if (vfs.is_dir(array_uri)):\n",
    "        vfs.remove_dir(array_uri)\n",
    "    print(f\"Iteration {i}: creating new array '{array_uri}'\")\n",
    "    ds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\")\n",
    "    ds\n",
    "    ds.create_dataset(enable_allele_count=True, enable_variant_stats=True)\n",
    "    \n",
    "    path_to_local_dir = \"temp_VCF_exports/untruncated_files\"\n",
    "    sample_list = [\"HG00096.bcf\", \"HG00097.bcf\", \"HG00099.bcf\", \"HG00100.bcf\", \"HG00101.bcf\"]\n",
    "    new_samples = [f\"{path_to_local_dir}/{s}\" for s in sample_list]\n",
    "    \n",
    "    with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n",
    "        ingest_samples_without_risk_of_duplication(new_samples)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    timing_results.append(elapsed_time)\n",
    "\n",
    "average_time = statistics.mean(timing_results)\n",
    "std_dev_time = statistics.stdev(timing_results)\n",
    "\n",
    "print(f\"It took {average_time:.2f} +/- {std_dev_time:.2f} s ({number_of_iterations} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1797a-4be7-4d41-8d6f-bb63380f504d",
   "metadata": {},
   "source": [
    "Then, we do the same for the same samples in the S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "341ba5e6-de32-44f9-a68b-0cab8003d581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 1: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 2: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 3: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 4: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 5: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 6: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 7: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 8: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 9: creating new array './temp_array_storage/ingestion_testing'\n",
      "It took 154.62 +/- 5.77 s (10 iterations)\n"
     ]
    }
   ],
   "source": [
    "timing_results = []\n",
    "number_of_iterations = 10\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    start_time = time.time()\n",
    "    if (vfs.is_dir(array_uri)):\n",
    "        vfs.remove_dir(array_uri)\n",
    "    print(f\"Iteration {i}: creating new array '{array_uri}'\")\n",
    "    ds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\")\n",
    "    ds\n",
    "    ds.create_dataset(enable_allele_count=True, enable_variant_stats=True)\n",
    "    \n",
    "    vcf_bucket = \"s3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1\"\n",
    "    sample_list = [\"HG00096.bcf\", \"HG00097.bcf\", \"HG00099.bcf\", \"HG00100.bcf\", \"HG00101.bcf\"]\n",
    "    new_samples = [f\"{vcf_bucket}/{s}\" for s in sample_list]\n",
    "\n",
    "    with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n",
    "        ingest_samples_without_risk_of_duplication(new_samples)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    timing_results.append(elapsed_time)\n",
    "\n",
    "average_time = statistics.mean(timing_results)\n",
    "std_dev_time = statistics.stdev(timing_results)\n",
    "\n",
    "print(f\"It took {average_time:.2f} +/- {std_dev_time:.2f} s ({number_of_iterations} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e12c9-1bd6-444d-8376-84ebe1209be9",
   "metadata": {},
   "source": [
    "These results are not unexpected, as they confirm what the TileDB-VCF tutorials already showed: it does take substantially longer time to ingest these files from S3. The difference was longer on this machine than the 15 extra seconds or so that the [tutorial estimated](https://tiledb-inc.github.io/TileDB-VCF/examples/tutorial_tiledbvcf_basics.html). I'd still be careful to extrapolate this to any general statement about local files versus S3 hosted files, but there seem to be a trend. Also to keep in mind is that this is the outcome of these tutorial files and this particular S3 bucket. Other cloud storage solutions might have other performance, based on user permissions and bandwidth limitations. \n",
    "\n",
    "Update: this [forum thread](https://forum.tiledb.com/t/s3-first-access-very-slow-with-3d-tiled-dense-array/416/) explains that it has to do with the limitations of how many objects S3 will return per request, meaning that the API typically will need to make multiple requests. So this seem to be a known bottleneck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f41432-061f-432a-be42-d5d08b50dd43",
   "metadata": {},
   "source": [
    "# 5. Does the ingestion time increase linearly with increasing sample count?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a90d7b-da80-45a5-9803-ef6191e21407",
   "metadata": {},
   "source": [
    "Previous results in the notebook seemed to imply that the ingestion time was linearly proportional to the number of ingested samples. To further investigate if this is the case, we can modify the last code block to time the ingestion of each sample. To do that, we will need to loop over each sample in the `new_samples` list and feed them one-by-one to `ingest_samples_without_risk_of_duplication()`. We will use the local files to test this, since it is proved to be much faster than the S3 bucket ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "b95d0aec-d2f7-486b-aa49-d1137b9a3c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 1: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 2: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 3: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 4: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 5: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 6: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 7: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 8: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 9: creating new array './temp_array_storage/ingestion_testing'\n",
      "Sample HG00096.bcf: 3.29 +/- 0.16 s (10 iterations)\n",
      "Sample HG00097.bcf: 3.38 +/- 0.24 s (10 iterations)\n",
      "Sample HG00099.bcf: 3.29 +/- 0.23 s (10 iterations)\n",
      "Sample HG00100.bcf: 3.41 +/- 0.27 s (10 iterations)\n",
      "Sample HG00101.bcf: 3.31 +/- 0.10 s (10 iterations)\n",
      "Total runtime: 16.82 +/- 0.67 s (10 iterations)\n"
     ]
    }
   ],
   "source": [
    "timing_results = []\n",
    "number_of_iterations = 10\n",
    "dict_elapsed_time_single_sample = {i: [] for i in range(len(new_samples))}\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    start_time = time.time()\n",
    "    if (vfs.is_dir(array_uri)):\n",
    "        vfs.remove_dir(array_uri)\n",
    "    print(f\"Iteration {i}: creating new array '{array_uri}'\")\n",
    "    ds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\")\n",
    "    ds\n",
    "    ds.create_dataset(enable_allele_count=True, enable_variant_stats=True)\n",
    "    \n",
    "    path_to_local_dir = \"temp_VCF_exports/untruncated_files\"\n",
    "    sample_list = [\"HG00096.bcf\", \"HG00097.bcf\", \"HG00099.bcf\", \"HG00100.bcf\", \"HG00101.bcf\"]\n",
    "    new_samples = [f\"{path_to_local_dir}/{s}\" for s in sample_list]\n",
    "\n",
    "    # Ingest the samples in the new_samples list one by one. Note that the function needs a list as input.\n",
    "    for j, sample in enumerate(new_samples):\n",
    "        start_single_sample = time.time()\n",
    "        with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n",
    "            ingest_samples_without_risk_of_duplication([sample])\n",
    "        end_single_sample = time.time()\n",
    "        elapsed_time_single_sample= end_single_sample - start_single_sample\n",
    "        dict_elapsed_time_single_sample[j].append(elapsed_time_single_sample)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    timing_results.append(elapsed_time)\n",
    "\n",
    "# Calculate average and standard deviation for each sample\n",
    "for j in range(len(sample_list)):\n",
    "    average_time = statistics.mean(dict_elapsed_time_single_sample[j])\n",
    "    std_dev_time = statistics.stdev(dict_elapsed_time_single_sample[j])\n",
    "    print(f\"Sample {sample_list[j]}: {average_time:.2f} +/- {std_dev_time:.2f} s ({number_of_iterations} iterations)\")\n",
    "\n",
    "# Calculate overall average and standard deviation\n",
    "total_average_time = statistics.mean(timing_results)\n",
    "total_std_dev_time = statistics.stdev(timing_results)\n",
    "print(f\"Total runtime: {total_average_time:.2f} +/- {total_std_dev_time:.2f} s ({number_of_iterations} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca900e4-1e08-4edf-9825-249adfab6c34",
   "metadata": {},
   "source": [
    "Interestingly, this took about twice as long compared to the code where the all samples were passed in a list once to the `ds.ingest_sample()`: 8 s versus 16 s in this case. Also, for this data, we can see that each sample took equally long to ingest (~3 s).\n",
    "\n",
    "There is clearly an extra overhead of looping over all samples and reinitiating the process. If this is mostly due to the python code, or how `ds.ingest_sample()` works is difficult to say from this small experiment, but it shows that if multiple samples are to be ingested, they should be sent to the method as one big list of samples instead of sample-by-sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c4c30-b13c-4da5-89e3-f8c1a09869b2",
   "metadata": {},
   "source": [
    "# 6. The case of multi-samples VCFs: some lessions and unexpected turns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5729ab62-86d3-4b4e-bff0-02fff87eb982",
   "metadata": {},
   "source": [
    "TileDB-VCF tools does not support ingestion of multi-sample VCF files. This is perhaps one of its biggest drawbacks in terms of useability and performance. This essentially means that the user will need to first split any multi-samples VCFs with another tool, which is a process that is known to take a long time (depending on the data and the machine).\n",
    "\n",
    "Let's simulate a multi-sample BCF by ingesting the single-samples files and then exporting a multi-sample file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "a00ae917-2d55-4d6c-b495-6bc247d5895e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully ingested sample: ['temp_VCF_exports/untruncated_files/HG00096.bcf', 'temp_VCF_exports/untruncated_files/HG00097.bcf', 'temp_VCF_exports/untruncated_files/HG00099.bcf', 'temp_VCF_exports/untruncated_files/HG00100.bcf', 'temp_VCF_exports/untruncated_files/HG00101.bcf']\n",
      "Ingested samples: ['temp_VCF_exports/untruncated_files/HG00096.bcf', 'temp_VCF_exports/untruncated_files/HG00097.bcf', 'temp_VCF_exports/untruncated_files/HG00099.bcf', 'temp_VCF_exports/untruncated_files/HG00100.bcf', 'temp_VCF_exports/untruncated_files/HG00101.bcf']\n",
      "CPU times: user 2min 33s, sys: 3.5 s, total: 2min 36s\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if (vfs.is_dir(array_uri)):\n",
    "    vfs.remove_dir(array_uri)\n",
    "ds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\")\n",
    "ds.create_dataset(enable_allele_count=True, enable_variant_stats=True)\n",
    "    \n",
    "path_to_local_dir = \"temp_VCF_exports/untruncated_files\"\n",
    "sample_list = [\"HG00096.bcf\", \"HG00097.bcf\", \"HG00099.bcf\", \"HG00100.bcf\", \"HG00101.bcf\"]\n",
    "new_samples = [f\"{path_to_local_dir}/{s}\" for s in sample_list]\n",
    "    \n",
    "ingest_samples_without_risk_of_duplication(new_samples)\n",
    "\n",
    "ds = tiledbvcf.Dataset(uri=array_uri, mode=\"r\")\n",
    "ds.export(\n",
    "    samples = ds.samples()[0:5],\n",
    "    merge = True,\n",
    "    output_format = 'b',\n",
    "    output_path = 'temp_VCF_exports/untruncated_files/combined_HG00096-101.bcf',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db8357-990d-4912-b7d8-d2973e9fc3e6",
   "metadata": {},
   "source": [
    "We can view the file to ensure that it has been correctly exported. (There are ~250 lines of `##` metadata headers in these VCFs, so we can omit them for the sake of readability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "baf92bd6-47bb-4878-9325-0ca577621bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tHG00096\tHG00097\tHG00099\tHG00100\tHG00101\n",
      "1\t10177\trs367896724\tA\tAC\t100\tPASS\tVT=INDEL;AA=|||unknown(NO_COVERAGE);DP=412608;END=10177;AN=8;AC=4\tGT\t1|0\t0|1\t0|1\t1|0\t./.\n",
      "1\t10352\trs555500075\tT\tTA\t100\tPASS\tVT=INDEL;AA=|||unknown(NO_COVERAGE);DP=444575;AC=5;AN=10;END=10352\tGT\t1|0\t1|0\t0|1\t0|1\t1|0\n",
      "1\t10616\trs376342519\tCCGCCGTTGCAAAGGCGCGCCG\tC\t100\tPASS\tVT=INDEL;DP=11825;AC=10;AN=10;END=10637\tGT\t1|1\t1|1\t1|1\t1|1\t1|1\n",
      "1\t13110\trs540538026\tG\tA\t100\tPASS\tVT=SNP;AA=g|||;DP=23422;AC=1;AN=2;END=13110\tGT\t./.\t1|0\t./.\t./.\t./.\n",
      "1\t13116\trs62635286\tT\tG\t100\tPASS\tVT=SNP;AA=t|||;DP=44680;AC=2;AN=4;END=13116\tGT\t./.\t1|0\t./.\t./.\t1|0\n",
      "1\t13118\trs200579949\tA\tG\t100\tPASS\tVT=SNP;AA=a|||;DP=42790;AC=2;AN=4;END=13118\tGT\t./.\t1|0\t./.\t./.\t1|0\n",
      "1\t14464\trs546169444\tA\tT\t100\tPASS\tVT=SNP;AA=a|||;DP=53522;AC=3;AN=4;END=14464\tGT\t1|1\t./.\t1|0\t./.\t./.\n",
      "1\t14599\trs531646671\tT\tA\t100\tPASS\tVT=SNP;AA=t|||;DP=64162;AC=2;AN=4;END=14599\tGT\t./.\t0|1\t1|0\t./.\t./.\n",
      "1\t14604\trs541940975\tA\tG\t100\tPASS\tVT=SNP;AA=a|||;DP=58462;AC=2;AN=4;END=14604\tGT\t./.\t0|1\t1|0\t./.\t./.\n"
     ]
    }
   ],
   "source": [
    "!bcftools view temp_VCF_exports/untruncated_files/combined_HG00096-101.bcf | grep -v \"^##\" | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a93776-8783-454a-a58e-6a823000596f",
   "metadata": {},
   "source": [
    "It looks good, but to make it more readable, let's import it to pandas and use the pretty notebook rendering of dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "582f2526-e4c9-4d86-bce5-9152e1b8b15d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>FILTER</th>\n",
       "      <th>INFO</th>\n",
       "      <th>FORMAT</th>\n",
       "      <th>HG00096</th>\n",
       "      <th>HG00097</th>\n",
       "      <th>HG00099</th>\n",
       "      <th>HG00100</th>\n",
       "      <th>HG00101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10177</td>\n",
       "      <td>rs367896724</td>\n",
       "      <td>A</td>\n",
       "      <td>AC</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=INDEL;AA=|||unknown(NO_COVERAGE);DP=412608;...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "      <td>0|1</td>\n",
       "      <td>0|1</td>\n",
       "      <td>1|0</td>\n",
       "      <td>./.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10352</td>\n",
       "      <td>rs555500075</td>\n",
       "      <td>T</td>\n",
       "      <td>TA</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=INDEL;AA=|||unknown(NO_COVERAGE);DP=444575;...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "      <td>1|0</td>\n",
       "      <td>0|1</td>\n",
       "      <td>0|1</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10616</td>\n",
       "      <td>rs376342519</td>\n",
       "      <td>CCGCCGTTGCAAAGGCGCGCCG</td>\n",
       "      <td>C</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=INDEL;DP=11825;AC=10;AN=10;END=10637</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|1</td>\n",
       "      <td>1|1</td>\n",
       "      <td>1|1</td>\n",
       "      <td>1|1</td>\n",
       "      <td>1|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13110</td>\n",
       "      <td>rs540538026</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=g|||;DP=23422;AC=1;AN=2;END=13110</td>\n",
       "      <td>GT</td>\n",
       "      <td>./.</td>\n",
       "      <td>1|0</td>\n",
       "      <td>./.</td>\n",
       "      <td>./.</td>\n",
       "      <td>./.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13116</td>\n",
       "      <td>rs62635286</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=t|||;DP=44680;AC=2;AN=4;END=13116</td>\n",
       "      <td>GT</td>\n",
       "      <td>./.</td>\n",
       "      <td>1|0</td>\n",
       "      <td>./.</td>\n",
       "      <td>./.</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>13118</td>\n",
       "      <td>rs200579949</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=a|||;DP=42790;AC=2;AN=4;END=13118</td>\n",
       "      <td>GT</td>\n",
       "      <td>./.</td>\n",
       "      <td>1|0</td>\n",
       "      <td>./.</td>\n",
       "      <td>./.</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>14464</td>\n",
       "      <td>rs546169444</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=a|||;DP=53522;AC=3;AN=4;END=14464</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|1</td>\n",
       "      <td>./.</td>\n",
       "      <td>1|0</td>\n",
       "      <td>./.</td>\n",
       "      <td>./.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>14599</td>\n",
       "      <td>rs531646671</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=t|||;DP=64162;AC=2;AN=4;END=14599</td>\n",
       "      <td>GT</td>\n",
       "      <td>./.</td>\n",
       "      <td>0|1</td>\n",
       "      <td>1|0</td>\n",
       "      <td>./.</td>\n",
       "      <td>./.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>14604</td>\n",
       "      <td>rs541940975</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=a|||;DP=58462;AC=2;AN=4;END=14604</td>\n",
       "      <td>GT</td>\n",
       "      <td>./.</td>\n",
       "      <td>0|1</td>\n",
       "      <td>1|0</td>\n",
       "      <td>./.</td>\n",
       "      <td>./.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #CHROM    POS           ID                     REF ALT  QUAL FILTER  \\\n",
       "0       1  10177  rs367896724                       A  AC   100   PASS   \n",
       "1       1  10352  rs555500075                       T  TA   100   PASS   \n",
       "2       1  10616  rs376342519  CCGCCGTTGCAAAGGCGCGCCG   C   100   PASS   \n",
       "3       1  13110  rs540538026                       G   A   100   PASS   \n",
       "4       1  13116   rs62635286                       T   G   100   PASS   \n",
       "5       1  13118  rs200579949                       A   G   100   PASS   \n",
       "6       1  14464  rs546169444                       A   T   100   PASS   \n",
       "7       1  14599  rs531646671                       T   A   100   PASS   \n",
       "8       1  14604  rs541940975                       A   G   100   PASS   \n",
       "\n",
       "                                                INFO FORMAT HG00096 HG00097  \\\n",
       "0  VT=INDEL;AA=|||unknown(NO_COVERAGE);DP=412608;...     GT     1|0     0|1   \n",
       "1  VT=INDEL;AA=|||unknown(NO_COVERAGE);DP=444575;...     GT     1|0     1|0   \n",
       "2            VT=INDEL;DP=11825;AC=10;AN=10;END=10637     GT     1|1     1|1   \n",
       "3        VT=SNP;AA=g|||;DP=23422;AC=1;AN=2;END=13110     GT     ./.     1|0   \n",
       "4        VT=SNP;AA=t|||;DP=44680;AC=2;AN=4;END=13116     GT     ./.     1|0   \n",
       "5        VT=SNP;AA=a|||;DP=42790;AC=2;AN=4;END=13118     GT     ./.     1|0   \n",
       "6        VT=SNP;AA=a|||;DP=53522;AC=3;AN=4;END=14464     GT     1|1     ./.   \n",
       "7        VT=SNP;AA=t|||;DP=64162;AC=2;AN=4;END=14599     GT     ./.     0|1   \n",
       "8        VT=SNP;AA=a|||;DP=58462;AC=2;AN=4;END=14604     GT     ./.     0|1   \n",
       "\n",
       "  HG00099 HG00100 HG00101  \n",
       "0     0|1     1|0     ./.  \n",
       "1     0|1     0|1     1|0  \n",
       "2     1|1     1|1     1|1  \n",
       "3     ./.     ./.     ./.  \n",
       "4     ./.     ./.     1|0  \n",
       "5     ./.     ./.     1|0  \n",
       "6     1|0     ./.     ./.  \n",
       "7     1|0     ./.     ./.  \n",
       "8     1|0     ./.     ./.  "
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = \"bcftools view temp_VCF_exports/untruncated_files/combined_HG00096-101.bcf | grep -v '^##' | head -n 10\"\n",
    "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "bcfdf = pd.read_csv(StringIO(result.stdout), sep='\\t')\n",
    "bcfdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950f451-098e-4d00-816a-ec183d9c0030",
   "metadata": {},
   "source": [
    "If we compare this to `HG00096.bcf` (below), we can see that in the combined VCF \n",
    "- a) there are sample headers for all samples,\n",
    "- b) there are, as expected, rows for variants that not present in HG00096.bcf\n",
    "- c) there are row like number 3 that only contain variants for one of the samples,\n",
    "- d) there are, in e.g. line 0, positions where the different sample have different genotypes e.g. 0|1 (phased diploid, first allele has REF variant, second allele has the first variant in ALT) and 1|0.\n",
    "\n",
    "From a quick inspection of these particular example files, I did not find any examples of an ALT column that has been updated with more variants after combining the samples. Nevertheless, when that happens, the ALT column values and the genotype call needs to be updated for all samples. Without knowing too much about how VCF merging software operate, I can only make a guess that the operation to read and potentially update each row is likely to be a computational burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "339e758a-5645-4f5f-9aac-ed457d6b0cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>FILTER</th>\n",
       "      <th>INFO</th>\n",
       "      <th>FORMAT</th>\n",
       "      <th>HG00096</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10177</td>\n",
       "      <td>rs367896724</td>\n",
       "      <td>A</td>\n",
       "      <td>AC</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=10177;AC=1;AN=2;DP=103152;AA=|||unknown(NO...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10352</td>\n",
       "      <td>rs555500075</td>\n",
       "      <td>T</td>\n",
       "      <td>TA</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=10352;AC=1;AN=2;DP=88915;AA=|||unknown(NO_...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10616</td>\n",
       "      <td>rs376342519</td>\n",
       "      <td>CCGCCGTTGCAAAGGCGCGCCG</td>\n",
       "      <td>C</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=10637;AC=2;AN=2;DP=2365;VT=INDEL</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14464</td>\n",
       "      <td>rs546169444</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=14464;AC=2;AN=2;DP=26761;AA=a|||;VT=SNP</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14930</td>\n",
       "      <td>rs75454623</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=14930;AC=1;AN=2;DP=42231;AA=a|||;VT=SNP</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>15211</td>\n",
       "      <td>rs78601809</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=15211;AC=1;AN=2;DP=32245;AA=t|||;VT=SNP</td>\n",
       "      <td>GT</td>\n",
       "      <td>0|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>15274</td>\n",
       "      <td>rs62636497</td>\n",
       "      <td>A</td>\n",
       "      <td>G,T</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=15274;AC=1,1;AN=2;DP=23255;AA=g|||;VT=SNP;...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>15820</td>\n",
       "      <td>rs2691315</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=15820;AC=1;AN=2;DP=14933;AA=t|||;VT=SNP;EX...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>15903</td>\n",
       "      <td>rs557514207</td>\n",
       "      <td>G</td>\n",
       "      <td>GC</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=15903;AC=1;AN=2;DP=7012;AA=ccc|CC|CCC|dele...</td>\n",
       "      <td>GT</td>\n",
       "      <td>0|1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #CHROM    POS           ID                     REF  ALT  QUAL FILTER  \\\n",
       "0       1  10177  rs367896724                       A   AC   100   PASS   \n",
       "1       1  10352  rs555500075                       T   TA   100   PASS   \n",
       "2       1  10616  rs376342519  CCGCCGTTGCAAAGGCGCGCCG    C   100   PASS   \n",
       "3       1  14464  rs546169444                       A    T   100   PASS   \n",
       "4       1  14930   rs75454623                       A    G   100   PASS   \n",
       "5       1  15211   rs78601809                       T    G   100   PASS   \n",
       "6       1  15274   rs62636497                       A  G,T   100   PASS   \n",
       "7       1  15820    rs2691315                       G    T   100   PASS   \n",
       "8       1  15903  rs557514207                       G   GC   100   PASS   \n",
       "\n",
       "                                                INFO FORMAT HG00096  \n",
       "0  END=10177;AC=1;AN=2;DP=103152;AA=|||unknown(NO...     GT     1|0  \n",
       "1  END=10352;AC=1;AN=2;DP=88915;AA=|||unknown(NO_...     GT     1|0  \n",
       "2               END=10637;AC=2;AN=2;DP=2365;VT=INDEL     GT     1|1  \n",
       "3        END=14464;AC=2;AN=2;DP=26761;AA=a|||;VT=SNP     GT     1|1  \n",
       "4        END=14930;AC=1;AN=2;DP=42231;AA=a|||;VT=SNP     GT     1|0  \n",
       "5        END=15211;AC=1;AN=2;DP=32245;AA=t|||;VT=SNP     GT     0|1  \n",
       "6  END=15274;AC=1,1;AN=2;DP=23255;AA=g|||;VT=SNP;...     GT     1|2  \n",
       "7  END=15820;AC=1;AN=2;DP=14933;AA=t|||;VT=SNP;EX...     GT     1|0  \n",
       "8  END=15903;AC=1;AN=2;DP=7012;AA=ccc|CC|CCC|dele...     GT     0|1  "
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = \"bcftools view temp_VCF_exports/untruncated_files/HG00096.bcf | grep -v '^##' | head -n 10\"\n",
    "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "bcfdf_HG00096 = pd.read_csv(StringIO(result.stdout), sep='\\t')\n",
    "bcfdf_HG00096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7120177-e51d-421c-a82d-615afc1252e4",
   "metadata": {},
   "source": [
    "Attempting to ingest the new multi-sample VCF will result in a TileDB-VCF error (as was known):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "6a4470e6-c014-42d9-b0a1-4be18e3f1be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to ingest sample '['temp_VCF_exports/untruncated_files/combined_HG00096-101.bcf']': TileDB-VCF exception: Combined VCFs are current not suppported\n",
      "Ingested samples: ['temp_VCF_exports/untruncated_files/combined_HG00096-101.bcf']\n",
      "CPU times: user 26 ms, sys: 80.2 ms, total: 106 ms\n",
      "Wall time: 151 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if (vfs.is_dir(array_uri)):\n",
    "    vfs.remove_dir(array_uri)\n",
    "ds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\")\n",
    "ds.create_dataset(enable_allele_count=True, enable_variant_stats=True)\n",
    "    \n",
    "path_to_local_dir = \"temp_VCF_exports/untruncated_files\"\n",
    "sample_list = [\"combined_HG00096-101.bcf\"]\n",
    "new_samples = [f\"{path_to_local_dir}/{s}\" for s in sample_list]\n",
    "    \n",
    "ingest_samples_without_risk_of_duplication(new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd230044-c272-42f2-b6a1-dbe056d77791",
   "metadata": {},
   "source": [
    "Instead, let's explore how long time it takes to split the multi-sample BCF back to single-sample BCFs, and then ingesting them. Of course, `bcftools` offers options how to do this. There is even a special plugin call `split`for it, and the `bcftools`version used in this conda environment (see Section 1) at the time of writing includes the plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793117ed-692a-4407-8705-84fd3e8fbee6",
   "metadata": {},
   "source": [
    "There are several options in `bcftools split`that can be used to preserve the different combined column values from the multi-sample VCF, but for now let's just make a default split, making one .bcf file per sample in the multi-sample BCF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "cdc60b0c-28aa-4147-87ac-0a3d8cac240a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.6 ms, sys: 88.4 ms, total: 109 ms\n",
      "Wall time: 3.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!bcftools +split temp_VCF_exports/untruncated_files/combined_HG00096-101.bcf -Ob -o temp_VCF_exports/untruncated_files/resplit_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216fa373-e705-4179-a02d-f5129f441b9d",
   "metadata": {},
   "source": [
    "For these files, which are small tutorial files, this was a fast operation. But this might take substantially longer for larger files with a higher sample number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "ec33a90c-3e9f-42e3-b3da-5b42eabbfb95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>FILTER</th>\n",
       "      <th>INFO</th>\n",
       "      <th>FORMAT</th>\n",
       "      <th>HG00096</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10177</td>\n",
       "      <td>rs367896724</td>\n",
       "      <td>A</td>\n",
       "      <td>AC</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=INDEL;AA=|||unknown(NO_COVERAGE);DP=412608;...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10352</td>\n",
       "      <td>rs555500075</td>\n",
       "      <td>T</td>\n",
       "      <td>TA</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=INDEL;AA=|||unknown(NO_COVERAGE);DP=444575;...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10616</td>\n",
       "      <td>rs376342519</td>\n",
       "      <td>CCGCCGTTGCAAAGGCGCGCCG</td>\n",
       "      <td>C</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=INDEL;DP=11825;AC=10;AN=10;END=10637</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13110</td>\n",
       "      <td>rs540538026</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=g|||;DP=23422;AC=1;AN=2;END=13110</td>\n",
       "      <td>GT</td>\n",
       "      <td>./.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13116</td>\n",
       "      <td>rs62635286</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=t|||;DP=44680;AC=2;AN=4;END=13116</td>\n",
       "      <td>GT</td>\n",
       "      <td>./.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>13118</td>\n",
       "      <td>rs200579949</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=a|||;DP=42790;AC=2;AN=4;END=13118</td>\n",
       "      <td>GT</td>\n",
       "      <td>./.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>14464</td>\n",
       "      <td>rs546169444</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=a|||;DP=53522;AC=3;AN=4;END=14464</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>14599</td>\n",
       "      <td>rs531646671</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=t|||;DP=64162;AC=2;AN=4;END=14599</td>\n",
       "      <td>GT</td>\n",
       "      <td>./.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>14604</td>\n",
       "      <td>rs541940975</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=a|||;DP=58462;AC=2;AN=4;END=14604</td>\n",
       "      <td>GT</td>\n",
       "      <td>./.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #CHROM    POS           ID                     REF ALT  QUAL FILTER  \\\n",
       "0       1  10177  rs367896724                       A  AC   100   PASS   \n",
       "1       1  10352  rs555500075                       T  TA   100   PASS   \n",
       "2       1  10616  rs376342519  CCGCCGTTGCAAAGGCGCGCCG   C   100   PASS   \n",
       "3       1  13110  rs540538026                       G   A   100   PASS   \n",
       "4       1  13116   rs62635286                       T   G   100   PASS   \n",
       "5       1  13118  rs200579949                       A   G   100   PASS   \n",
       "6       1  14464  rs546169444                       A   T   100   PASS   \n",
       "7       1  14599  rs531646671                       T   A   100   PASS   \n",
       "8       1  14604  rs541940975                       A   G   100   PASS   \n",
       "\n",
       "                                                INFO FORMAT HG00096  \n",
       "0  VT=INDEL;AA=|||unknown(NO_COVERAGE);DP=412608;...     GT     1|0  \n",
       "1  VT=INDEL;AA=|||unknown(NO_COVERAGE);DP=444575;...     GT     1|0  \n",
       "2            VT=INDEL;DP=11825;AC=10;AN=10;END=10637     GT     1|1  \n",
       "3        VT=SNP;AA=g|||;DP=23422;AC=1;AN=2;END=13110     GT     ./.  \n",
       "4        VT=SNP;AA=t|||;DP=44680;AC=2;AN=4;END=13116     GT     ./.  \n",
       "5        VT=SNP;AA=a|||;DP=42790;AC=2;AN=4;END=13118     GT     ./.  \n",
       "6        VT=SNP;AA=a|||;DP=53522;AC=3;AN=4;END=14464     GT     1|1  \n",
       "7        VT=SNP;AA=t|||;DP=64162;AC=2;AN=4;END=14599     GT     ./.  \n",
       "8        VT=SNP;AA=a|||;DP=58462;AC=2;AN=4;END=14604     GT     ./.  "
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = \"bcftools view temp_VCF_exports/untruncated_files/resplit_files/HG00096.bcf | grep -v '^##'| head -n 10\"\n",
    "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "bcfdf_HG00096_resplit = pd.read_csv(StringIO(result.stdout), sep='\\t')\n",
    "bcfdf_HG00096_resplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a070d3-5729-4261-8f66-731ae6a655e8",
   "metadata": {},
   "source": [
    "So are these two version of this small selection of the dataframe the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "cc77525a-cacb-4c3e-97fd-e52c4467c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the dataframes from before and after the merge-and-split are equal: False\n"
     ]
    }
   ],
   "source": [
    "df_comparison = bcfdf_HG00096.equals(bcfdf_HG00096_resplit)\n",
    "print(f\"Are the dataframes from before and after the merge-and-split are equal: {df_comparison}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7676ab09-e526-418e-9b4a-f984b93c0b74",
   "metadata": {},
   "source": [
    "No they are not, since the empty genotypes were not filtered in the split. Also, the INFO column is sorted in a different order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "c30addaa-63a1-40bf-a0df-1de7d04d63b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>FILTER</th>\n",
       "      <th>INFO</th>\n",
       "      <th>FORMAT</th>\n",
       "      <th>HG00096</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10177</td>\n",
       "      <td>rs367896724</td>\n",
       "      <td>A</td>\n",
       "      <td>AC</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=10177;AC=1;AN=2;DP=103152;AA=|||unknown(NO...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10352</td>\n",
       "      <td>rs555500075</td>\n",
       "      <td>T</td>\n",
       "      <td>TA</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=10352;AC=1;AN=2;DP=88915;AA=|||unknown(NO_...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10616</td>\n",
       "      <td>rs376342519</td>\n",
       "      <td>CCGCCGTTGCAAAGGCGCGCCG</td>\n",
       "      <td>C</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=10637;AC=2;AN=2;DP=2365;VT=INDEL</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14464</td>\n",
       "      <td>rs546169444</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=14464;AC=2;AN=2;DP=26761;AA=a|||;VT=SNP</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14930</td>\n",
       "      <td>rs75454623</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=14930;AC=1;AN=2;DP=42231;AA=a|||;VT=SNP</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>15211</td>\n",
       "      <td>rs78601809</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=15211;AC=1;AN=2;DP=32245;AA=t|||;VT=SNP</td>\n",
       "      <td>GT</td>\n",
       "      <td>0|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>15274</td>\n",
       "      <td>rs62636497</td>\n",
       "      <td>A</td>\n",
       "      <td>G,T</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=15274;AC=1,1;AN=2;DP=23255;AA=g|||;VT=SNP;...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>15820</td>\n",
       "      <td>rs2691315</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=15820;AC=1;AN=2;DP=14933;AA=t|||;VT=SNP;EX...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>15903</td>\n",
       "      <td>rs557514207</td>\n",
       "      <td>G</td>\n",
       "      <td>GC</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>END=15903;AC=1;AN=2;DP=7012;AA=ccc|CC|CCC|dele...</td>\n",
       "      <td>GT</td>\n",
       "      <td>0|1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #CHROM    POS           ID                     REF  ALT  QUAL FILTER  \\\n",
       "0       1  10177  rs367896724                       A   AC   100   PASS   \n",
       "1       1  10352  rs555500075                       T   TA   100   PASS   \n",
       "2       1  10616  rs376342519  CCGCCGTTGCAAAGGCGCGCCG    C   100   PASS   \n",
       "3       1  14464  rs546169444                       A    T   100   PASS   \n",
       "4       1  14930   rs75454623                       A    G   100   PASS   \n",
       "5       1  15211   rs78601809                       T    G   100   PASS   \n",
       "6       1  15274   rs62636497                       A  G,T   100   PASS   \n",
       "7       1  15820    rs2691315                       G    T   100   PASS   \n",
       "8       1  15903  rs557514207                       G   GC   100   PASS   \n",
       "\n",
       "                                                INFO FORMAT HG00096  \n",
       "0  END=10177;AC=1;AN=2;DP=103152;AA=|||unknown(NO...     GT     1|0  \n",
       "1  END=10352;AC=1;AN=2;DP=88915;AA=|||unknown(NO_...     GT     1|0  \n",
       "2               END=10637;AC=2;AN=2;DP=2365;VT=INDEL     GT     1|1  \n",
       "3        END=14464;AC=2;AN=2;DP=26761;AA=a|||;VT=SNP     GT     1|1  \n",
       "4        END=14930;AC=1;AN=2;DP=42231;AA=a|||;VT=SNP     GT     1|0  \n",
       "5        END=15211;AC=1;AN=2;DP=32245;AA=t|||;VT=SNP     GT     0|1  \n",
       "6  END=15274;AC=1,1;AN=2;DP=23255;AA=g|||;VT=SNP;...     GT     1|2  \n",
       "7  END=15820;AC=1;AN=2;DP=14933;AA=t|||;VT=SNP;EX...     GT     1|0  \n",
       "8  END=15903;AC=1;AN=2;DP=7012;AA=ccc|CC|CCC|dele...     GT     0|1  "
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcfdf_HG00096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f91794-fd0b-4734-9dad-e8fc7f0368e6",
   "metadata": {},
   "source": [
    "Apparently, `split`does not have an option for filtering out lines that only contain empty genotypes, but regular `bcftools view`does. To wrap the command in a python variable, we need to escape some quotes, so let's use a `\"\"\"` block to handle this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "17db625f-593f-48c6-8d8a-34e7519072f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>FILTER</th>\n",
       "      <th>INFO</th>\n",
       "      <th>FORMAT</th>\n",
       "      <th>HG00096</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10177</td>\n",
       "      <td>rs367896724</td>\n",
       "      <td>A</td>\n",
       "      <td>AC</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=INDEL;AA=|||unknown(NO_COVERAGE);DP=412608;...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10352</td>\n",
       "      <td>rs555500075</td>\n",
       "      <td>T</td>\n",
       "      <td>TA</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=INDEL;AA=|||unknown(NO_COVERAGE);DP=444575;...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10616</td>\n",
       "      <td>rs376342519</td>\n",
       "      <td>CCGCCGTTGCAAAGGCGCGCCG</td>\n",
       "      <td>C</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=INDEL;DP=11825;AC=10;AN=10;END=10637</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14464</td>\n",
       "      <td>rs546169444</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=a|||;DP=53522;AC=3;AN=4;END=14464</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14930</td>\n",
       "      <td>rs75454623</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=a|||;DP=211155;AC=5;AN=10;END=14930</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>15211</td>\n",
       "      <td>rs78601809</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>VT=SNP;AA=t|||;DP=161225;AC=5;AN=10;END=15211</td>\n",
       "      <td>GT</td>\n",
       "      <td>0|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>15274</td>\n",
       "      <td>rs62636497</td>\n",
       "      <td>A</td>\n",
       "      <td>G,T</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>MULTI_ALLELIC;VT=SNP;AA=g|||;DP=116275;AC=3,7;...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>15820</td>\n",
       "      <td>rs2691315</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>EX_TARGET;VT=SNP;AA=t|||;DP=44799;AC=3;AN=6;EN...</td>\n",
       "      <td>GT</td>\n",
       "      <td>1|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>15903</td>\n",
       "      <td>rs557514207</td>\n",
       "      <td>G</td>\n",
       "      <td>GC</td>\n",
       "      <td>100</td>\n",
       "      <td>PASS</td>\n",
       "      <td>EX_TARGET;VT=INDEL;AA=ccc|CC|CCC|deletion;DP=2...</td>\n",
       "      <td>GT</td>\n",
       "      <td>0|1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #CHROM    POS           ID                     REF  ALT  QUAL FILTER  \\\n",
       "0       1  10177  rs367896724                       A   AC   100   PASS   \n",
       "1       1  10352  rs555500075                       T   TA   100   PASS   \n",
       "2       1  10616  rs376342519  CCGCCGTTGCAAAGGCGCGCCG    C   100   PASS   \n",
       "3       1  14464  rs546169444                       A    T   100   PASS   \n",
       "4       1  14930   rs75454623                       A    G   100   PASS   \n",
       "5       1  15211   rs78601809                       T    G   100   PASS   \n",
       "6       1  15274   rs62636497                       A  G,T   100   PASS   \n",
       "7       1  15820    rs2691315                       G    T   100   PASS   \n",
       "8       1  15903  rs557514207                       G   GC   100   PASS   \n",
       "\n",
       "                                                INFO FORMAT HG00096  \n",
       "0  VT=INDEL;AA=|||unknown(NO_COVERAGE);DP=412608;...     GT     1|0  \n",
       "1  VT=INDEL;AA=|||unknown(NO_COVERAGE);DP=444575;...     GT     1|0  \n",
       "2            VT=INDEL;DP=11825;AC=10;AN=10;END=10637     GT     1|1  \n",
       "3        VT=SNP;AA=a|||;DP=53522;AC=3;AN=4;END=14464     GT     1|1  \n",
       "4      VT=SNP;AA=a|||;DP=211155;AC=5;AN=10;END=14930     GT     1|0  \n",
       "5      VT=SNP;AA=t|||;DP=161225;AC=5;AN=10;END=15211     GT     0|1  \n",
       "6  MULTI_ALLELIC;VT=SNP;AA=g|||;DP=116275;AC=3,7;...     GT     1|2  \n",
       "7  EX_TARGET;VT=SNP;AA=t|||;DP=44799;AC=3;AN=6;EN...     GT     1|0  \n",
       "8  EX_TARGET;VT=INDEL;AA=ccc|CC|CCC|deletion;DP=2...     GT     0|1  "
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = \"\"\"\n",
    "    bcftools view -e 'GT=\"./.\"' temp_VCF_exports/untruncated_files/resplit_files/HG00096.bcf | grep -v '^##' | head -n 10\n",
    "    \"\"\"\n",
    "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "bcfdf_HG00096_resplit_filtered = pd.read_csv(StringIO(result.stdout), sep='\\t')\n",
    "bcfdf_HG00096_resplit_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887bbdc2-6441-4497-b42d-176e529151ba",
   "metadata": {},
   "source": [
    "Because of the different sorting in INFO, the dataframe difference remains. But since it seems like the values in INFO are preserved, we can accept this for the sake of this experiment. (Down the line, and with a non-tutorial dataset, it would be important to ensure that the splitting recreates files identical to the files from before the merge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "04a69b6e-2df0-48a6-9fce-d50c90c9f81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the dataframes from before and after the merge-and-split are equal: False\n"
     ]
    }
   ],
   "source": [
    "df_comparison = bcfdf_HG00096.equals(bcfdf_HG00096_resplit_filtered)\n",
    "print(f\"Are the dataframes from before and after the merge-and-split are equal: {df_comparison}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b420e8-5258-476b-986f-83eab96b12a1",
   "metadata": {},
   "source": [
    "We can do a quick sanity-check to verify that the number of lines are different in the two files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "27f9ec7f-30b9-48c7-ae37-ae8c939085d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320726\n"
     ]
    }
   ],
   "source": [
    "!bcftools view temp_VCF_exports/untruncated_files/HG00096.bcf | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "4b371884-ec77-4413-a808-7fb4aeb37949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560069\n"
     ]
    }
   ],
   "source": [
    "!bcftools view temp_VCF_exports/untruncated_files/resplit_files/HG00096.bcf | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3483183-1ccc-485e-b025-ccdb17f355aa",
   "metadata": {},
   "source": [
    "And here it is clear that the filtering step results in the same number of lines as the original. Very nice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "149058d4-5cec-465a-96db-910eada86e23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320726\n"
     ]
    }
   ],
   "source": [
    "!bcftools view -e 'GT=\"./.\"' temp_VCF_exports/untruncated_files/resplit_files/HG00096.bcf | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f851e1b-7649-40f5-b1ac-1f376418bc8f",
   "metadata": {},
   "source": [
    "What about the file size? We need to save the filtered the files to disk for later operations anyway, so let's do that already and include them in the file size comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "e8faa058-ecfc-4d55-ae95-a040212a0a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_local_dir = \"temp_VCF_exports/untruncated_files/resplit_files\"\n",
    "sample_list = [\"HG00096.bcf\", \"HG00097.bcf\", \"HG00099.bcf\", \"HG00100.bcf\", \"HG00101.bcf\"]\n",
    "new_samples = [f\"{path_to_local_dir}/{s}\" for s in sample_list]\n",
    "for file in new_samples:\n",
    "    basename = file.rstrip(\".bcf\")\n",
    "    !bcftools view -e 'GT=\"./.\"' {file} -Ob -o {basename}_filtered.bcf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a938ce-71e1-4804-85d1-6beb0f9236c4",
   "metadata": {},
   "source": [
    " The split unfiltered files are, as expected from the extra line count, larger. They are more in the size range of the merged multi-sample file, which make sense, since there cannot be more lines than in the multi-sample file.\n",
    "\n",
    "The split and filtered files are much closer in file size to the original files. The differences can probably be attributed things like how the the `##` metadata preamble was not touched in the splitting and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "675305b6-0df6-4331-b243-500f2a648fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Resplit files, unfiltered for empty genotypes:\n",
      "temp_VCF_exports/untruncated_files/resplit_files/HG00101.bcf\t11.14 Mb\n",
      "temp_VCF_exports/untruncated_files/resplit_files/HG00100.bcf\t11.14 Mb\n",
      "temp_VCF_exports/untruncated_files/resplit_files/HG00099.bcf\t11.14 Mb\n",
      "temp_VCF_exports/untruncated_files/resplit_files/HG00097.bcf\t11.14 Mb\n",
      "temp_VCF_exports/untruncated_files/resplit_files/HG00096.bcf\t11.14 Mb\n",
      "- Resplit files, filtered for empty genotypes:\n",
      "temp_VCF_exports/untruncated_files/resplit_files/HG00099_filtered.bcf\t6.32 Mb\n",
      "temp_VCF_exports/untruncated_files/resplit_files/HG00101_filtered.bcf\t6.37 Mb\n",
      "temp_VCF_exports/untruncated_files/resplit_files/HG00100_filtered.bcf\t6.42 Mb\n",
      "temp_VCF_exports/untruncated_files/resplit_files/HG00097_filtered.bcf\t6.47 Mb\n",
      "temp_VCF_exports/untruncated_files/resplit_files/HG00096_filtered.bcf\t6.32 Mb\n",
      "- Original files, does not contain empty genotypes:\n",
      "temp_VCF_exports/untruncated_files/HG00101.bcf\t6.19 Mb\n",
      "temp_VCF_exports/untruncated_files/HG00100.bcf\t6.24 Mb\n",
      "temp_VCF_exports/untruncated_files/HG00099.bcf\t6.14 Mb\n",
      "temp_VCF_exports/untruncated_files/combined_HG00096-101.bcf\t11.77 Mb\n",
      "temp_VCF_exports/untruncated_files/HG00097.bcf\t6.28 Mb\n",
      "temp_VCF_exports/untruncated_files/HG00096.bcf\t6.14 Mb\n"
     ]
    }
   ],
   "source": [
    "file_path = \"temp_VCF_exports/untruncated_files/resplit_files\"\n",
    "vcf_files = glob.glob(os.path.join(file_path, \"*.bcf\"))\n",
    "vcf_files = [f for f in vcf_files if not f.endswith(\"filtered.bcf\")]\n",
    "\n",
    "print(\"- Resplit files, unfiltered for empty genotypes:\")\n",
    "for file in vcf_files:\n",
    "    file_size_bytes = os.path.getsize(file)\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "    print(f\"{file}\\t{file_size_mb:.2f} Mb\")\n",
    "\n",
    "vcf_files = glob.glob(os.path.join(file_path, \"*filtered.bcf\"))\n",
    "\n",
    "print(\"- Resplit files, filtered for empty genotypes:\")\n",
    "for file in vcf_files:\n",
    "    file_size_bytes = os.path.getsize(file)\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "    print(f\"{file}\\t{file_size_mb:.2f} Mb\")\n",
    "    \n",
    "print(\"- Original files, does not contain empty genotypes:\")\n",
    "file_path = \"temp_VCF_exports/untruncated_files/\"\n",
    "vcf_files = glob.glob(os.path.join(file_path, \"*.bcf\"))\n",
    "\n",
    "for file in vcf_files:\n",
    "    file_size_bytes = os.path.getsize(file)\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "    print(f\"{file}\\t{file_size_mb:.2f} Mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb865471-93cc-4abe-b11e-109ee889a7d6",
   "metadata": {},
   "source": [
    "Let's ingest the split files and check the performance. First we need to index the new files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "17bed91c-edf6-44c4-b723-41b57f5adcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_local_dir = \"temp_VCF_exports/untruncated_files/resplit_files\"\n",
    "sample_list = [\"HG00096.bcf\", \"HG00097.bcf\", \"HG00099.bcf\", \"HG00100.bcf\", \"HG00101.bcf\"]\n",
    "new_samples = [f\"{path_to_local_dir}/{s}\" for s in sample_list]\n",
    "for file in new_samples:\n",
    "    !bcftools index {file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "55ade852-b8bb-45bb-8abc-d416fe188ea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 1: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 2: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 3: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 4: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 5: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 6: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 7: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 8: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 9: creating new array './temp_array_storage/ingestion_testing'\n",
      "It took 3.49 +/- 0.13 s (10 iterations)\n"
     ]
    }
   ],
   "source": [
    "timing_results = []\n",
    "number_of_iterations = 10\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    start_time = time.time()\n",
    "    if (vfs.is_dir(array_uri)):\n",
    "        vfs.remove_dir(array_uri)\n",
    "    print(f\"Iteration {i}: creating new array '{array_uri}'\")\n",
    "    ds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\")\n",
    "    ds\n",
    "    ds.create_dataset(enable_allele_count=True, enable_variant_stats=True)\n",
    "    \n",
    "    path_to_local_dir = \"temp_VCF_exports/untruncated_files/resplit_files\"\n",
    "    sample_list = [\"HG00096.bcf\", \"HG00097.bcf\", \"HG00099.bcf\", \"HG00100.bcf\", \"HG00101.bcf\"]\n",
    "    new_samples = [f\"{path_to_local_dir}/{s}\" for s in sample_list]\n",
    "    \n",
    "    with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n",
    "        ingest_samples_without_risk_of_duplication([sample])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    timing_results.append(elapsed_time)\n",
    "\n",
    "average_time = statistics.mean(timing_results)\n",
    "std_dev_time = statistics.stdev(timing_results)\n",
    "\n",
    "print(f\"It took {average_time:.2f} +/- {std_dev_time:.2f} s ({number_of_iterations} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c577fe-83e5-4930-a1c5-7fac4e0f2ca8",
   "metadata": {},
   "source": [
    "The ingestion time actually seem to be slightly faster for the files that have gone through the merge-split than for the original files (3 s versus 8 s). Question is if this is due to the difference in sorting of e.g. the INFO column, or if the data of the files acctually are different. The above cell was run on the non-filtered files (i.e. lines with empty genotype were still present). This could be a future line of investigation\n",
    "\n",
    "For curiosity, what about the time it takes to ingest the split and filtered files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b72c26-f582-47f7-8111-e167557390c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_local_dir = \"temp_VCF_exports/untruncated_files/resplit_files\"\n",
    "sample_list = [\"HG00096_filtered.bcf\", \"HG00097_filtered.bcf\", \"HG00099_filtered.bcf\", \"HG00100_filtered.bcf\", \"HG00101_filtered.bcf\"]\n",
    "new_samples = [f\"{path_to_local_dir}/{s}\" for s in sample_list]\n",
    "for file in new_samples:\n",
    "    !bcftools index {file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "6373b288-f3be-4183-b29f-4bd9223b6191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 1: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 2: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 3: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 4: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 5: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 6: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 7: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 8: creating new array './temp_array_storage/ingestion_testing'\n",
      "Iteration 9: creating new array './temp_array_storage/ingestion_testing'\n",
      "It took 3.32 +/- 0.07 s (10 iterations)\n"
     ]
    }
   ],
   "source": [
    "timing_results = []\n",
    "number_of_iterations = 10\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    start_time = time.time()\n",
    "    if (vfs.is_dir(array_uri)):\n",
    "        vfs.remove_dir(array_uri)\n",
    "    print(f\"Iteration {i}: creating new array '{array_uri}'\")\n",
    "    ds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\")\n",
    "    ds\n",
    "    ds.create_dataset(enable_allele_count=True, enable_variant_stats=True)\n",
    "\n",
    "    path_to_local_dir = \"temp_VCF_exports/untruncated_files/resplit_files\"\n",
    "    sample_list = [\"HG00096_filtered.bcf\", \"HG00097_filtered.bcf\", \"HG00099_filtered.bcf\", \"HG00100_filtered.bcf\", \"HG00101_filtered.bcf\"]\n",
    "    new_samples = [f\"{path_to_local_dir}/{s}\" for s in sample_list]\n",
    "\n",
    "    with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n",
    "        ingest_samples_without_risk_of_duplication([sample])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    timing_results.append(elapsed_time)\n",
    "\n",
    "average_time = statistics.mean(timing_results)\n",
    "std_dev_time = statistics.stdev(timing_results)\n",
    "\n",
    "print(f\"It took {average_time:.2f} +/- {std_dev_time:.2f} s ({number_of_iterations} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b7e74-74d2-4d58-8d73-4b4ee0d6257c",
   "metadata": {},
   "source": [
    "No, it does not seem to make any difference at all for these tutorial files. Nevertheless, these results make sense in the light of how TileDB-VCF is based on storing data in sparse arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cbc51f-9a97-4852-b0b0-f964e1b267e4",
   "metadata": {},
   "source": [
    "For the sake of the experiment and for reuse in future testing of multi-sample VCFs, we can wrap up the whole multi-sample process splitting and ingestion, and time it. The function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "6287efd3-d76a-4b60-9904-d3b77146b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_local_dir = \"temp_VCF_exports/untruncated_files\"\n",
    "\n",
    "def ingest_multi_sample_file(multi_sample_file, dir_split_samples = None):\n",
    "    if not dir_split_samples:\n",
    "        dir_split_samples = \"temp_VCF_exports/untruncated_files/resplit_files\"\n",
    "        \n",
    "    # Read the header from the BCF/VCF and save it to a python variable\n",
    "    view_header_command = f\"bcftools view {multi_sample_file} | grep -v '^##' | head -n 1\"\n",
    "    try:\n",
    "        view_header_result = subprocess.run(view_header_command, shell=True, capture_output=True, text=True)\n",
    "        output = view_header_result.stdout\n",
    "        # Make a list of all the samples names \n",
    "        sample_list = output.split('\\t')[output.split('\\t').index(\"FORMAT\") + 1:]\n",
    "        sample_list[-1] = sample_list[-1].rstrip('\\n')\n",
    "        sample_list = [sample + \".bcf\" for sample in sample_list]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "    # Split the samples into seprate BCF files\n",
    "    split_command = f\"bcftools +split {multi_sample_file} -Ob -o {dir_split_samples}\"\n",
    "    try:\n",
    "        subprocess.run(split_command, shell=True, check=True, capture_output=True, text=True)\n",
    "        print(\"bcftools +split command executed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    new_samples = [f\"{dir_split_samples}/{s}\" for s in sample_list]\n",
    "\n",
    "    print(\"Indexing the split files.\")\n",
    "    for file in new_samples:\n",
    "        try:\n",
    "            subprocess.run(f\"bcftools index {file}\", shell=True, check=True, capture_output=True, text=True)\n",
    "        except Exception as e:\n",
    "             print(f\"Error: {e}\")\n",
    "\n",
    "    ingest_samples_without_risk_of_duplication(new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e958f9-75de-4e41-9b26-ed3ee915e6eb",
   "metadata": {},
   "source": [
    "And running it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "880db6ac-96cc-48c9-978d-4c22cd68a2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcftools +split command executed successfully.\n",
      "Indexing the split files.\n",
      "Successfully ingested sample: ['temp_VCF_exports/untruncated_files/resplit_files/HG00096.bcf', 'temp_VCF_exports/untruncated_files/resplit_files/HG00097.bcf', 'temp_VCF_exports/untruncated_files/resplit_files/HG00099.bcf', 'temp_VCF_exports/untruncated_files/resplit_files/HG00100.bcf', 'temp_VCF_exports/untruncated_files/resplit_files/HG00101.bcf']\n",
      "Ingested samples: ['temp_VCF_exports/untruncated_files/resplit_files/HG00096.bcf', 'temp_VCF_exports/untruncated_files/resplit_files/HG00097.bcf', 'temp_VCF_exports/untruncated_files/resplit_files/HG00099.bcf', 'temp_VCF_exports/untruncated_files/resplit_files/HG00100.bcf', 'temp_VCF_exports/untruncated_files/resplit_files/HG00101.bcf']\n",
      "CPU times: user 12.6 s, sys: 2.99 s, total: 15.6 s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if (vfs.is_dir(array_uri)):\n",
    "    vfs.remove_dir(array_uri)\n",
    "ds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\")\n",
    "ds.create_dataset(enable_allele_count=True, enable_variant_stats=True)\n",
    "\n",
    "ingest_multi_sample_file(\"temp_VCF_exports/untruncated_files/combined_HG00096-101.bcf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b8b06-a3b3-4e68-8d44-75a157daf30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12d6c387-b3d3-42ac-a42f-53ee7be92e1c",
   "metadata": {},
   "source": [
    "This process, of course, takes longer than the ingestion of the five single-samples files (15 s versus 8 s), but it would be interesting to try this with larger files with more samples to see how this scales.\n",
    "\n",
    "In all, this notebook resulted many interesting observations about how data ingestion works for TileDB-VCF. As mentioned throughout the notes, the next logical step would be to try to scale this up with a much bigger dataset and see how it performs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
